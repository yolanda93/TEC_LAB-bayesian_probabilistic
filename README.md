# M√©todos Bayesianos y Programaci√≥n Probabil√≠stica

## Introducci√≥n (TODO)
* ¬øQu√© es?
La **estad√≠stica bayesiana** es un subconjunto del campo de la estad√≠stica en la que la evidencia sobre el verdadero estado del mundo se expresa **en t√©rminos de grados de creencia, es decir, trata la modelizaci√≥n de la incertidumbre**
* ¬øPor qu√© es interesante?

Con este enfoque podemos estimar la 'probabilidad de que una hip√≥tesis sea cierta' (incertidumbre de la hip√≥tesis), que llevado al campo de Machine Learning nos permite representar un modelo ML como el conjunto de asunciones o hip√≥tesis que realiza y la probabilidad de que esas asunciones se acerquen a la realidad.

Sin embargo, esta incertidumbre actualmente no se modela ya que **los modelos de ML se basan m√°s en estad√≠stica frecuentista** que a diferencia de la bayesiana realiza inferencias sin expresar esta incertidumbre al respecto, evaluando las hip√≥tesis en t√©rminos absolutos usando las evidencias disponibles y de acuerdo a unos ciertos umbrales de aceptaci√≥n de tal hip√≥tesis (p-values, CI, etc)

C√≥mo resultado de aplicar este enfoque podr√≠amos obtener la distribuci√≥n (posterior) de los par√°metros del modelo y no s√≥lo puntos de estimaci√≥n como se puede observar en la siguiente f√≥rmula:

---

## Teorema de Bayes

La formulaci√≥n matem√°tica del enfoque bayesiano utiliza [el teorema de bayes](https://es.wikipedia.org/wiki/Teorema_de_Bayes), descrito mediante la siguiente f√≥rmula:

![](img/formula_bayes.png)

En la f√≥rmula matem√°tica anterior podemos ver un ejemplo de c√≥mo calcular estas probabilidades, usando el teorema de bayes. Este teorema por definici√≥n trata de calcular las probabilidades subjetivas que puede tomar un determinado suceso cuando hemos recibido alg√∫n tipo de informaci√≥n previa. Para ello, se calcula la probabilidad a posteriori P(A|B), en base a las probabilidades a priori o P(A) y la probabilidad de que se d√© el suceso B si la hip√≥tesis A es cierta, P(B|A). [1]


---


Actualmente se est√°n utilizando multitud de t√©cnicas para obtenci√≥n de indicadores y m√©tricas dentro de las tecnolog√≠as de la informaci√≥n, que van desde estad√≠stica b√°sica hasta t√©cnicas de aprendizaje autom√°tico. As√≠, se obtienen unos valores que aportan una informaci√≥n que ayuda en las estimaciones y toma de decisiones. No obstante, la naturaleza de gran parte de los datos y, sobre todo, de gran parte de las m√©tricas que se quieren obtener es probabil√≠stica. 

Las t√©cnicas m√°s tradicionales, orientadas desde el punto de vista frecuentista, nos permiten obtener medidas descriptivas de los datos, tales como la mediai, la moda o la varianza de los datos. Sin embargo, obteniendo estos datos como descriptivos, podemos estar cayendo en una serie de asunciones de las que no somos conscientes: si asumimos que una media y una varianza describen nuestros datos, seguramente sea porque estamos suponiendo que nuestros datos siguen una distribuci√≥n normal, o gausiana.

Es decir, estamos haciendo unas suposiciones a priori sobre nuestros datos, lo que nos lleva a unos resultados a posteriori sobre ellos. Pero podemos cambiar esas asuciones por otras cualesquiera, y el modelo estad√≠stico que permite trabajar con esta serie de asunciones es la estad√≠stica bayesiana [METER ENLACE].

Una vez entramos en este enfoque, podemos ir mas all√°. Un modelo de red neuronal se puede determinar como, dados unos priors, como son la arquitectura y los datos de entrada, obtenemos a posteriori unos pesos para la neuronas.[REVISAR]

As√≠, la inferencia bayesiana abre los l√≠mites de las aproximaciones frecuentistas y propociona un campo m√°s amplio de trabajo, donde no s√≥lo obtenemos unos resultados, si no que obtenemos informaci√≥n asociada a esos resultados.

Con esta idea en mente, se han realizado una serie de trabajos orientados a obtener predicciones y la incertidumbre asociada a estas predicciones. De este modo, quienes interpreten los resultados no s√≥lo obtendr√°n un valor, si no tambi√©n una m√©trica que informe acerca de lo preciso, v√°lido o importante que sea este valor, dependiendo de c√≥mo se haya aplicado la m√©trica.

Estos trabajos parten de la determinaci√≥n del grado de confianza en un resultado mediante t√©cnicas m√°s comunes y espec√≠ficas hasta la utilizaci√≥n de una t√©cnica avanzanda generalista utilizando redes neuronales.

### Aplicaciones de la inferencia bayesiana

Una de las grandes aplicaciones de la estad√≠stica bayesiana es la [estimaci√≥n de la incertidumbre](bayesian_deep_learning/uncertainty_estimation). Se ha realizado un amplio trabajo en este √°rea, realizandose un estudio del problema, t√©cnicas existentes y formas de abordarlo.  


### El problema de la inferencia Bayesiana

La inferencia bayesiana puede convertirse en un problema intratable o con una complejidad computacional muy alta dependiendo de las **asunciones tomadas por el modelo y la dimensionalidad**. Por ello todas las t√©cnicas se basan en m√©todos de aproximaci√≥n basandose en el conocimiento del problema para la modelizaci√≥n del prior o mediante la aproximaci√≥n de la distribuci√≥n a posteriori

En este sentido se han estudiado t√©cnicas que permiten aproximar su resultado:

- **Sampling-based**: MCMC Markov Chain Monte Carlo. Las cadenas de Markov de Monte Carlo es un m√©todo n√∫merico de aproximaci√≥n que tiene como objetivo aproximar una distribuci√≥n de probabilidad determinada. La idea del algoritmo es simular una cadena de Markov cuya distribuci√≥n estacionaria se aproxime a la distribuci√≥n a posteriori del modelo, de esta forma somos capaces de obtener una aproximaci√≥n de la distribucci√≥n. Destacan 2 m√©todos:
	- Metropolis-Hasting 
	- Gibbs Sampling
	
- **Aproximation-based**: Variatonal Inference (VI). Este algoritmo aproxima la distribuci√≥n a posteriori a trav√©s de funciones de distribuci√≥n m√°s sencillas, de esta forma transforma el problema a uno de optimizaci√≥n donde buscamos minimizar la diferencia entre la funci√≥n a posteriori y la aproximaci√≥n. Esta diferencia se mide a trav√©s de la divergencia de Kullback-Leibler.


## Trabajo realizado
* [Aprendizaje profundo bayesiano](bayesian_deep_learning)
  * [Estimaci√≥n de la incertidumbre](bayesian_deep_learning/uncertainty_estimation)
* Redes Bayesianas
* Procesos Gausianos
* Causalidad
  * [Recursos en Google Drive](https://drive.google.com/drive/folders/1uefX12ZtAieVE3SVsTqI94E8s5wXOs07)
  * [Modelos de Ecuaciones Estructurales](https://github.com/beeva/TEC_LAB-structural_equation_modeling)
  * [Causalidad "vs" Machine Learning](https://github.com/beeva/TEC_LAB-causality_vs_machine_learning)
* üîß  Herramientas y librer√≠as
  * [Pyro](https://github.com/next-samuelmunoz/bayprob) (Pytorch library)
  * [√Årboles de decisi√≥n bayesianos](https://github.com/beeva/TEC_LAB-bayesian_decision_trees)
* üó£Ô∏è Comunicaci√≥n
  * PPT - [Estimaci√≥n de la incertidumbre](https://docs.google.com/presentation/d/1mRkL54FNAwC0YNSKmbeWWg-IJNR2ch6oCLktIXDMjfc)
  * PPT - [AI desde un nuevo punto de vista: Bayesianos](https://docs.google.com/presentation/d/158Wi28rWwBFuqM1bmjjy03PLX83ssA8p3vq_Op9HL7M)
  * BBVA Next Technologies Blog (15/01/2020) - [Bayesianos, viendo la inteligencia artificial desde otro prisma](https://www.bbvanexttechnologies.com/bayesianos-viendo-la-inteligencia-artificial-desde-otro-prisma/)
  
  
  
## Referencias
* Introduction to Bayesian data analysis - Youtube
  * [Part 1: What is Bayes?](https://www.youtube.com/watch?v=3OJEae7Qb_o)
  * [Part 2: Why use Bayes?](https://www.youtube.com/watch?v=mAUwjSo5TJE)
  * [Part 3: How to do Bayes?](https://www.youtube.com/watch?v=Ie-6H_r7I5A)
* [Visualizaci√≥n gr√°fica de conceptos b√°sicos de probabilidad](https://seeing-theory.brown.edu/)
* [Is bayesian the most brilliant thing ever?](https://www.youtube.com/watch?v=HumFmLu3CJ8) - Youtube, NIPS 2016
* [Bayesian probability theory](http://users.ics.aalto.fi/harri/thesis/valpola_thesis/node12.html) - Polytechnica scandinavica

