## Mixture Models
En est√° p√°gina se explica **los modelos de mixturas** como soluci√≥n t√©cnica para aproximar distribuciones heterog√©neas de la variable respuesta. Este es el caso en el que sabemos que los datos u observaciones provienen de fuentes o procesos diferentes conocidos.


### Indice de contenidos
- [Introducci√≥n a la t√©cnica](#introduccion)
- [Mixture Density Networks](#mdn)
- [MLE - Maximum Likelihood Estimation](#MLE) 
- [Experimentos y conclusiones](#Experimentos-y-conclusiones) 

<a name="introduccion"></a>
## Introducci√≥n

Un **modelo de mixturas** es un modelo probabil√≠stico que nos permite representar la presencia de sub-poblaciones de la poblaci√≥n general. Esta representaci√≥n de sub-poblaciones nos va a permitir construir un estimador m√°s robusto en el caso en el que la distribuci√≥n de la variable respuesta sea heterog√©nea


<a name="mdn"></a>
### Mixture Density Networks

Las **redes de densidad mixta** (Bishop, 1994) es un tipo de red que combina las redes convencionales con el concepto de modelo de mixturas. En este modelo, la s√°lida de la DNN hace la estimaci√≥n de par√°metros para la familia de distribuciones o componentes seleccionadas las cuales se suman teniendo en cuenta el coeficiente de mezcla ‚ç∫ para obtener finalmente una distribucci√≥n condicional het√©rogena de y respecto a la entrada: 

<p align="center"><img src="./img/MDN.png" height="160" alt="Mixture Density Network" /></p>
<p align="center">Mixture Density Network</p>

Formalmente la probabilidad condicionada de una red de mixturas tiene la siguiente forma:

<p align="center"><img src="./img/mdn_formula.png" height="70" alt="Formula MDN" /></p>
<p align="center">Mixture Density Network</p>

En esta f√≥rmula los par√°metros tiene la siguiente sem√°ntica:

* **c se corresponde con el √≠ndice de la correspondiente mixtura**. Hay hasta C componentes de mixtura (e.g. distribuciones) por salida, siendo un parametro seleccionable.
* **‚ç∫ es el coeficiente de mezcla**. Para entender este coeficiente podemos imaginarnos los controles deslizantes que controlan la mezcla de C salidas diferentes de audio. Este par√°metro esta condicionado por la entrada x.
* **ùíü  esta es la correspondiente distribuci√≥n de entrada a ser mezclada**. La distribuci√≥n puede ser elegida atendiendo al tipo de aplicaci√≥n.
* **Œª son los par√°metros de la distribuci√≥n ùíü**. En el caso denotamos ùíü como una distribuci√≥n gausiana, estos parametros corresponderian a Œª1 ser√≠a la media condicional mean Œº(x) y 
Œª2 la desviaci√≥n est√°ndar œÉ(x). Las distribuciones pueden tener distinto n√∫mero de par√°metros (e.g.: Bernoulli and Chi2 tienen 1 par√°metro, Beta tiene 2, y la gaussiana truncada tiene hasta 4 par√°metros) Estos son par√°metros que forman tambi√©n la salida de la red.

<a name="MLE"></a>
### MLE - Maximum Likelihood Estimation

El algoritmo de MLE o m√°xima verosimilitud nos permite obtener los par√°metros del modelo o distribuci√≥n que maximizan la probabibilidad de obtener unos datos dados.

Referencia - [Ejemplo de c√°lculo de MLE para la implementaci√≥n de la funci√≥n de p√©rdida](https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f#:~:text=%E2%80%9CA%20method%20of%20estimating%20the,observed%20data%20is%20most%20probable.%E2%80%9D&text=Let's%20say%20we%20have%20some,that%20it%20is%20normally%20distributed)


### Aplicaciones

Entre las **aplicaciones m√°s destacadas** se encuentra la de Apple‚Äôs Siri en iOS 11 para reconocimiento de voz[2]. En [3] se puede ver su aplicaci√≥n en generaci√≥n de manuscritos y Amazon Forecast lo tiene dentro su suite de algoritmos incluidos en su plataforma.

### Ventajas y desventajas

* *Ventajas* 

Permite estimar distribuciones heterog√©neas de la variable respuesta

* *Desventajas* 

El tama√±o de la s√°lida de la red creada por la capa final de la MDN es (c+2)* m, d√≥nde c es la dimensi√≥n de s√°lida de la red convencional y m el n√∫mero de mixturas que estamos usando. Esto supone un incremento considerable en la dimensi√≥n de s√°lida respecto a la red convencional lo que puede volverlas muy inestables.

### Experimentos y conclusiones
   
Este m√©todo en contraposici√≥n con lo validado en el Exp.I de estimaci√≥n de incertidumbre al vuelo presentan las siguientes ventajas que se pueden resumir a mayor libertad en la definici√≥n del prior:

 - Permite modelar facilmente que el ruido provenga de distintas familias de distribucciones 
 - Pueden modelar ruido multimodal, es decir, que no s√≥lo provenga de una sola distribuci√≥n si no de la suma de varias distribucciones de la misma familia con distintos par√°metros. Este prior, sin embargo, tambi√©n esta implicito en el exp.I y no es f√°cilmente modificable.
 - Tienen m√°s soporte, es decir, el m√©todo est√° m√°s comunmente aceptado. 

Se puede encontrar un [notebook](experiments/V0.1.6-real_datasets/uncertainty_prediction_house_prices_mdn.ipynb) con esta t√©cnica aplicadas a un dataset para la estimaci√≥n de incertidumbre.

En detalle, se puede encontrar un [notebook de redes de densidad mixta (MDN)](experiments/V3.0.0-mixture_density_networks), donde se puede encontrar una implementaci√≥n y el detalle de las conclusiones.


#### Referencias

[1] https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca

[2] Siri Team, Deep Learning for Siri‚Äôs Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis (2017)

[3] Alex Graves, Generating Sequences With Recurrent Neural Networks (2014)
